from cutlass.utils import TensorMapUpdateMode
import cutlass.torch as cutlass_torch
from typing import List, Tuple
import functools
{{gen_defines()}}
# ---- Import GroupedGemm implementation, copied on PyTorch build from Cutlass repository: cutlass/examples/python/CuTeDSL/blackwell/grouped_gemm.py ----
from torch._inductor.kernel.vendored_templates.cutedsl_grouped_gemm import GroupedGemmKernel

cache = {}

def get_cache_key(kernel_name: str, input_a: torch.Tensor, input_b: torch.Tensor, out_ptr0: torch.Tensor) -> str:
    """
    Returns a string key for caching kernel executors based on kernel name,
    shapes, strides, and dtypes of input/output tensors.
    """
    a_shape = ",".join(map(str, input_a.shape))
    a_stride = ",".join(map(str, input_a.stride()))
    b_shape = ",".join(map(str, input_b.shape))
    b_stride = ",".join(map(str, input_b.stride()))
    c_shape = ",".join(map(str, out_ptr0.shape))
    c_stride = ",".join(map(str, out_ptr0.stride()))

    key = (
        f"{kernel_name}"
        f"|A={a_shape}:{a_stride}:{input_a.dtype}"
        f"|B={b_shape}:{b_stride}:{input_b.dtype}"
        f"|C={c_shape}:{c_stride}:{out_ptr0.dtype}"
    )
    return key


@cute.kernel
def build_group_ptrs_from_bases_kernel(
    base_A_u64: cutlass.Int64,  # device addr of input_a (bytes)
    base_B_u64: cutlass.Int64,  # device addr of input_b (bytes)
    base_C_u64: cutlass.Int64,  # device addr of Output (bytes)
    offs: cute.Tensor,  # [G], cutlass.Int32/64 cumulative
    K: cutlass.Constexpr,
    N: cutlass.Constexpr,
    sizeof_element: cutlass.Int32,  # bytes
    # -------- STRIDES (in ELEMENTS) --------
    stride_A_m_elems: cutlass.Constexpr,  # A.stride(0)
    stride_A_k_elems: cutlass.Constexpr,  # A.stride(1)
    stride_B0_elems: cutlass.Constexpr,  # B.stride(0)
    stride_Bk_elems: cutlass.Constexpr,  # B.stride(1)
    stride_Bn_elems: cutlass.Constexpr,  # B.stride(2)
    stride_C_m_elems: cutlass.Constexpr,  # C.stride(0)
    stride_C_n_elems: cutlass.Constexpr,  # C.stride(1)
    # -------- OUTPUTS --------
    out_ptrs: cute.Tensor,  # [G,3] cutlass.Int64: (A_ptr, B_ptr, C_ptr)
    out_problem: cute.Tensor,  # [G,4] cutlass.Int32: (m_g, n, k, 1)
    out_strides_abc: cute.Tensor,  # [G,3,2] cutlass.Int32 [[A_m,A_k],[B_n,B_k],[C_m,C_n]]
):
    tidx, _, _ = cute.arch.thread_idx()
    g = tidx

    m_beg_i32 = 0
    if g > 0:
        m_beg_i32 = offs[g - 1]
    m_end_i32 = offs[g]
    m_g_i32 = m_end_i32 - m_beg_i32

    a_byte_off = (
        cutlass.Int64(m_beg_i32) * stride_A_m_elems * cutlass.Int64(sizeof_element)
    )
    c_byte_off = (
        cutlass.Int64(m_beg_i32) * stride_C_m_elems * cutlass.Int64(sizeof_element)
    )
    b_byte_off = cutlass.Int64(g) * stride_B0_elems * cutlass.Int64(sizeof_element)

    # ---- pointers ----
    out_ptrs[g, 0] = base_A_u64 + a_byte_off
    out_ptrs[g, 1] = base_B_u64 + b_byte_off
    out_ptrs[g, 2] = base_C_u64 + c_byte_off

    # ---- (m, n, k, 1) ----
    out_problem[g, 0] = m_g_i32
    out_problem[g, 1] = N
    out_problem[g, 2] = K
    out_problem[g, 3] = cutlass.Int32(1)

    # ---- strides ----
    out_strides_abc[g, 0, 0] = cutlass.Int32(stride_A_m_elems)
    out_strides_abc[g, 0, 1] = cutlass.Int32(stride_A_k_elems)
    out_strides_abc[g, 1, 0] = cutlass.Int32(stride_Bn_elems)
    out_strides_abc[g, 1, 1] = cutlass.Int32(stride_Bk_elems)
    out_strides_abc[g, 2, 0] = cutlass.Int32(stride_C_m_elems)
    out_strides_abc[g, 2, 1] = cutlass.Int32(stride_C_n_elems)


@cute.jit
def launch_build_group_ptrs_from_bases(
    base_A_u64: cutlass.Int64,
    base_B_u64: cutlass.Int64,
    base_C_u64: cutlass.Int64,
    offs: cute.Tensor,
    G: cutlass.Constexpr,
    K: cutlass.Constexpr,
    N: cutlass.Constexpr,
    sizeof_element: cutlass.Constexpr,
    stride_A_m_elems: cutlass.Constexpr,
    stride_A_k_elems: cutlass.Constexpr,
    stride_B0_elems: cutlass.Constexpr,
    stride_Bk_elems: cutlass.Constexpr,
    stride_Bn_elems: cutlass.Constexpr,
    stride_C_m_elems: cutlass.Constexpr,
    stride_C_n_elems: cutlass.Constexpr,
    out_ptrs: cute.Tensor,  # [G,3] cutlass.Int64
    out_problem: cute.Tensor,  # [G,4] cutlass.Int32
    out_strides_abc: cute.Tensor,  # [3,2] cutlass.Int32
    stream: cuda.CUstream,
):
    build_group_ptrs_from_bases_kernel(
        base_A_u64,
        base_B_u64,
        base_C_u64,
        offs,
        K,
        N,
        sizeof_element,
        stride_A_m_elems,
        stride_A_k_elems,
        stride_B0_elems,
        stride_Bk_elems,
        stride_Bn_elems,
        stride_C_m_elems,
        stride_C_n_elems,
        out_ptrs,
        out_problem,
        out_strides_abc,
    ).launch(grid=(1, 1, 1), block=(G, 1, 1), stream=stream)


{{def_kernel("input_a", "input_b", "input_a_offs")}}
    stream = cuda.CUstream(stream)

    # TODO(nikhilap) figure out a way to get rid of this
    input_b = input_b.transpose(1, 2)

    sumM, K = input_a.shape
    G, N, Kb = input_b.shape

    dev = input_a.device

    base_A_u64 = int(input_a.data_ptr())
    base_B_u64 = int(input_b.data_ptr())
    base_C_u64 = int(out_ptr0.data_ptr())

    ptrs_t = torch.empty((G, 3), device=dev, dtype=torch.int64)
    probs_t = torch.empty((G, 4), device=dev, dtype=torch.int32)
    strides_t = torch.empty((G, 3, 2), device=dev, dtype=torch.int32)
    ptrs = from_dlpack(ptrs_t)
    probs = from_dlpack(probs_t)
    strides = from_dlpack(strides_t)

    cache_key = get_cache_key("{{kernel_name}}", input_a, input_b, out_ptr0)

    if cache_key not in cache:
        sizeof_element = int(input_a.element_size())
        sA_m, sA_k = map(int, input_a.stride())
        sB_0, sB_n, sB_k = map(int, input_b.stride())
        sC_m, sC_n = map(int, out_ptr0.stride())

        build_group_ptrs_jit_executor = cute.compile(
            launch_build_group_ptrs_from_bases,
            base_A_u64=base_A_u64,
            base_B_u64=base_B_u64,
            base_C_u64=base_C_u64,
            offs=from_dlpack(input_a_offs),
            G=int(G),
            K=int(K),
            N=int(N),
            sizeof_element=sizeof_element,
            stride_A_m_elems=sA_m,
            stride_A_k_elems=sA_k,
            stride_B0_elems=sB_0,
            stride_Bk_elems=sB_k,
            stride_Bn_elems=sB_n,
            stride_C_m_elems=sC_m,
            stride_C_n_elems=sC_n,
            out_ptrs=ptrs,
            out_problem=probs,
            out_strides_abc=strides,
            stream=stream,
        )


        build_group_ptrs_jit_executor(
            base_A_u64=base_A_u64,
            base_B_u64=base_B_u64,
            base_C_u64=base_C_u64,
            offs=from_dlpack(input_a_offs),
            out_ptrs=ptrs,
            out_problem=probs,
            out_strides_abc=strides,
            stream=stream,
        )

        # --- Tensormap workspace per SM ---
        hw = cutlass.utils.HardwareInfo()
        sm_count = hw.get_max_active_clusters(1)
        max_active_clusters = hw.get_max_active_clusters(CLUSTER_M * CLUSTER_N)

        num_tensormap_buffers = sm_count
        tensormap_shape = (
            num_tensormap_buffers,
            GroupedGemmKernel.num_tensormaps,
            GroupedGemmKernel.bytes_per_tensormap // 8,
        )
        tensormap_zeros = torch.zeros(
            tensormap_shape, device=dev, dtype=torch.int64
        )
        tensormap_cute = from_dlpack(tensormap_zeros)

        # --- Total clusters ---
        def compute_total_num_clusters(
            problem_sizes_mnkl: List[tuple[int, int, int, int]],
            cluster_tile_shape_mn: tuple[int, int],
        ) -> int:
            total_num_clusters = 0
            for m, n, _, _ in problem_sizes_mnkl:
                num_clusters_mn = tuple(
                    (x + y - 1) // y for x, y in zip((m, n), cluster_tile_shape_mn)
                )
                total_num_clusters += functools.reduce(lambda x, y: x * y, num_clusters_mn)
            return total_num_clusters

        # Compute cluster tile shape
        def compute_cluster_tile_shape(
            mma_tiler_mn: tuple[int, int],
            cluster_shape_mn: tuple[int, int],
            use_2cta_instrs: bool,
        ) -> tuple[int, int]:
            cta_tile_shape_mn = list(mma_tiler_mn)
            if use_2cta_instrs:
                cta_tile_shape_mn[0] = cta_tile_shape_mn[0] // 2
            return tuple(x * y for x, y in zip(cta_tile_shape_mn, cluster_shape_mn))

        cluster_tile_shape_mn = compute_cluster_tile_shape(
            (TILE_M, TILE_N), (CLUSTER_M, CLUSTER_N), bool(USE_2_CTA)
        )

        # TODO(nikhilap) remove read into probs_t on the host
        total_num_clusters = int(compute_total_num_clusters(probs_t, cluster_tile_shape_mn))

        grouped_gemm = GroupedGemmKernel(
            acc_dtype=ACC_DTYPE,
            use_2cta_instrs=USE_2_CTA,
            mma_tiler_mn=(TILE_M, TILE_N),
            cluster_shape_mn=(CLUSTER_M, CLUSTER_N),
            tensormap_update_mode=TENSORMAP_UPDATE_MODE,
        )

        group_gemm_jit_executor = cute.compile(
            grouped_gemm,
            from_dlpack(input_a.unsqueeze(-1)),
            from_dlpack(input_b[0].unsqueeze(-1)),
            from_dlpack(out_ptr0.unsqueeze(-1)),
            G,
            probs,
            strides,
            ptrs,
            total_num_clusters,
            tensormap_cute,
            max_active_clusters,
            stream,
        )

        group_gemm_jit_executor(
            from_dlpack(input_a.unsqueeze(-1)),
            from_dlpack(input_b[0].unsqueeze(-1)),
            from_dlpack(out_ptr0.unsqueeze(-1)),
            probs,
            strides,
            ptrs,
            tensormap_cute,
            stream,
        )

        cache[cache_key] = (build_group_ptrs_jit_executor, group_gemm_jit_executor, total_num_clusters, tensormap_shape)
    else:
        build_group_ptrs_jit_executor, group_gemm_jit_executor, total_num_clusters, tensormap_shape = cache[cache_key]

        build_group_ptrs_jit_executor(
            base_A_u64=base_A_u64,
            base_B_u64=base_B_u64,
            base_C_u64=base_C_u64,
            offs=from_dlpack(input_a_offs),
            out_ptrs=ptrs,
            out_problem=probs,
            out_strides_abc=strides,
            stream=stream,
        )

        tensormap_zeros = torch.zeros(
            tensormap_shape, device=dev, dtype=torch.int64
        )
        tensormap_cute = from_dlpack(tensormap_zeros)

        group_gemm_jit_executor(
            from_dlpack(input_a.unsqueeze(-1)),
            from_dlpack(input_b[0].unsqueeze(-1)),
            from_dlpack(out_ptr0.unsqueeze(-1)),
            probs,
            strides,
            ptrs,
            tensormap_cute,
            stream,
        )
